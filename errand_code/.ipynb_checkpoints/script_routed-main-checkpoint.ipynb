{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T01:41:43.351426Z",
     "start_time": "2019-02-26T01:41:43.348257Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define my shopping list\n",
    "shopping_list = ['chicken thighs', 'apples','shampoo', 'chorizo', 'venison', 'lemongrass', 'bacon'\n",
    "                 'lemongrass','manicure', 'bicycle chain','milk','oranges','rotate tires','vermicelli','fertilizer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T01:41:44.614663Z",
     "start_time": "2019-02-26T01:41:44.436991Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extra imports (for plots, etc.)\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-26T01:46:48.850Z"
    }
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import requests\n",
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Local imports\n",
    "import routedtools\n",
    "from config import * # i don't know why regular import isn't working...\n",
    "\n",
    "# from itertools import permutations, repeat\n",
    "# Only need to do this one time to enable lemmatization - let it download to default place\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# A) Connect to appropriate Yelp database\n",
    "print('Establishing database connection....')\n",
    "conn = psycopg2.connect(database='yelp_small', user='brooks', password='davidson', host='localhost')\n",
    "\n",
    "# B) Load (or rebuild) word vector dict\n",
    "try:\n",
    "    print('Loading GloVe vectors...')\n",
    "    glove_dict = pickle.load(open( \"./resources/glove_dict.p\", \"rb\" ) )\n",
    "except:\n",
    "    print('Rebuilding GloVe vectors...')\n",
    "    glove_dict = routedtools.build_word_vector_matrix('./glovesets/glove.6B.50d.txt', 500000)\n",
    "    pickle.dump( glove_dict, open( \"./resources/glove_dict.p\", \"wb\" ) )\n",
    "    print('saved as ./resources/glove_dict.p')\n",
    "# C) Get salient word lists from all Phoenix businesses (i.e. preprocess, TF-IDF normalize, and rank)\n",
    "try:\n",
    "    print('Loading Yelp wordlist ...')\n",
    "    topwords_df = pickle.load(open( \"./resources/bestwords.p\", \"rb\" ) )\n",
    "except:\n",
    "    print('Rebuilding Yelp wordlist (this is gonna take a sec) ...')\n",
    "    stopwords = routedtools.get_stop_words(\"./resources/stopwords.txt\")\n",
    "    tname = 't_phoenix'\n",
    "    topwords_df = routedtools.get_salient_terms(conn, tname, stopwords, glove_dict)\n",
    "    pickle.dump(topwords_df, open( \"./resources/bestwords.p\", \"wb\" ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-26T01:46:14.989Z"
    }
   },
   "outputs": [],
   "source": [
    "start_address = '84 W Cypress St Phoenix, AZ 85003'\n",
    "\n",
    "url = 'https://maps.googleapis.com/maps/api/geocode/json?address='+start_address.replace(' ', '+')+\\\n",
    "    '&key='+api_key\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-26T01:41:51.609Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: start with an address. (PHX is relatively overrepresented in dataset, so start there)\n",
    "start_address = '84 W Cypress St Phoenix, AZ 85003'\n",
    "\n",
    "url = 'https://maps.googleapis.com/maps/api/geocode/json?address='+start_address.replace(' ', '+')+\\\n",
    "    '&key='+api_key\n",
    "r = requests.get(url)\n",
    "\n",
    "# Pull out latitude/longitude\n",
    "start_lat = r.json()['results'][0]['geometry']['location']['lat']\n",
    "start_long = r.json()['results'][0]['geometry']['location']['lng']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T01:52:54.919091Z",
     "start_time": "2019-02-26T01:52:54.916073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://maps.googleapis.com/maps/api/geocode/json?address=84+W+Cypress+St+Phoenix,+AZ+85003&key=AIzaSyCYKfVCUd11rHqauyK0an2kqoY7EL4Okm0\n"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "start_address = '84 W Cypress St Phoenix, AZ 85003'\n",
    "\n",
    "url = 'https://maps.googleapis.com/maps/api/geocode/json?address='+start_address.replace(' ', '+')+\\\n",
    "    '&key='+api_key\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T23:06:44.184068Z",
     "start_time": "2019-02-25T23:06:33.458351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stores found within 5.0 mile radius of address: 1605\n",
      "processing chicken thighs\n",
      "processing apples\n",
      "processing shampoo\n",
      "processing chorizo\n",
      "processing venison\n",
      "processing lemongrass\n",
      "processing baconlemongrass\n",
      "(skipped this item.)\n",
      "processing manicure\n",
      "processing bicycle chain\n",
      "processing milk\n",
      "processing oranges\n",
      "processing rotate tires\n",
      "processing vermicelli\n",
      "processing fertilizer\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(routedtools)\n",
    "# - - - - - - - - - - - - - -\n",
    "\n",
    "\n",
    "# Step 2: Select for businesses/reviews within a given radius of starting address\n",
    "radius = (5 / 69) # in miles, converted to degrees lat/long\n",
    "min_length = 6000 # Min number of characters for a given business, ~1000 words\n",
    "\n",
    "nearby_stores = routedtools.get_nearby(start_lat, start_long, radius, \n",
    "                                       't_phoenix', 't_review_phx', conn, min_length)[0]\n",
    "topwords_df = topwords_df.loc[nearby_stores['business_id']] # Filter for relevant word lists\n",
    "\n",
    "print(\"Number of stores found within\", radius*69, \n",
    "      \"mile radius of address:\", nearby_stores.shape[0])\n",
    "\n",
    "\n",
    "# Step 3: Map my query term(s) to GloVe vectors - calculate cosine distances to corresponding points for businesses\n",
    "match_df = routedtools.get_matches(shopping_list, glove_dict, nearby_stores, topwords_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T23:11:54.690694Z",
     "start_time": "2019-02-25T23:11:54.446175Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f632e702253f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     top_info = routedtools.knapsack_optimize(match_df, shopping_list, \n\u001b[0;32m---> 25\u001b[0;31m                                              nearby_stores, start_lat,start_long, weights) \n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Step 5: brute-force solve the resultant Traveling Salesman Problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     nodes = routedtools.make_nodes(top_info['latitude'].tolist(),top_info['longitude'].tolist(),\n",
      "\u001b[0;32m~/Documents/Projects/errand_router/errand_code/routedtools.py\u001b[0m in \u001b[0;36mknapsack_optimize\u001b[0;34m(match_df, shopping_list, nearby_stores, start_lat, start_long, weights)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;31m# A) Build candidate list - *every* store with a score (on some item) above a threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mfull_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melement\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlist_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatch_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'business_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0mfull_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melement\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlist_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatch_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item_copy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mid_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(routedtools)\n",
    "# - - - - - - - - - - - - - -\n",
    "\n",
    "# Steps 4 & 5: turn match list into a routing\n",
    "# Aggregate weights\" - [SCALING SCORE MAY CHANGE AS MATCHING ALGORITHM CHANGES]\n",
    "a = [0.1, 100, 0]  # weight of distance penalty\n",
    "b = [10, 1, 1]  # weight of matching penalty\n",
    "c = [0, 1, 10]  # weight of rating penalty\n",
    "\n",
    "top_choices = []\n",
    "top_addresses = []\n",
    "top_items=[]\n",
    "top_stars=[]\n",
    "all_routes = []\n",
    "all_starts = [0]\n",
    "\n",
    "all_mapurls = []\n",
    "all_imgurls = []\n",
    "\n",
    "for i in range(len(a)):\n",
    "    # Step 4: Build \"best\" list of stores -> reformulate my optimization as a quadratic knapsack problem\n",
    "    weights = [a[i],b[i],c[i]]\n",
    "    top_info = routedtools.knapsack_optimize(match_df, shopping_list, \n",
    "                                             nearby_stores, start_lat,start_long, weights) \n",
    "    # Step 5: brute-force solve the resultant Traveling Salesman Problem.\n",
    "    nodes = routedtools.make_nodes(top_info['latitude'].tolist(),top_info['longitude'].tolist(),\n",
    "                                   start_lat,start_long)\n",
    "    cost, route = routedtools.find_shortest_route(nodes, len(nodes))\n",
    "    print('Shortest route: {}'.format(route))\n",
    "    print('Travel cost   : {}'.format(cost*69))\n",
    "    # Build maps URLS\n",
    "    maps_url,img_url = routedtools.build_urls(start_address, route, top_info, start_lat,start_long)\n",
    "    \n",
    "    # Append new results into output vars                      \n",
    "    top_choices.extend(top_info['name'].values)\n",
    "    top_addresses.extend(top_info['address'].values)\n",
    "    top_items.extend(top_info['items'].values)\n",
    "    top_stars.extend(top_info['stars'].values)\n",
    "    all_routes.extend([route[1:-1]])\n",
    "    all_starts.append(top_info.shape[0]+all_starts[-1])\n",
    "    all_mapurls.append(maps_url)\n",
    "    all_imgurls.append(img_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (OPTIONAL) show plots about reviews\n",
    "# Show (log) distribution of all reviews\n",
    "tmp_review = routedtools.get_nearby(start_lat, start_long, radius, 't_phoenix', 't_review_phx', conn, 0)[1]\n",
    "plt.hist(np.log10(tmp_review['text'].str.len()),np.linspace(2,6))\n",
    "plt.axvline(x=np.log10(min_length),color='#000000')\n",
    "plt.show()\n",
    "\n",
    "# Show remaining category contents (can modify SQL query to not drop restaurants if we want to see those)\n",
    "all_tags = ((nearby_stores['categories']).str.cat(sep=', ')).split(',')\n",
    "tag_counts = Counter(all_tags)\n",
    "df = (pd.DataFrame.from_dict(tag_counts, orient='index')).sort_values(by=[0],ascending=False)\n",
    "df.reset_index(level=0, inplace=True)\n",
    "df.rename(columns={0: \"count\", \"index\": \"category\"},inplace=True)\n",
    "\n",
    "df1 = df[df['count']>40];\n",
    "plt.figure(figsize=[12,3])\n",
    "plt.bar(df1['category'].tolist(),df1['count'])\n",
    "plt.xticks(rotation=270)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show TSP scatterplot\n",
    "xy1 = np.array(list(nodes.values()))\n",
    "x1= xy1[:,0]\n",
    "y1=xy1[:,1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax.scatter(x1,y1,120,'w',alpha=0.5,edgecolors='b',)\n",
    "for idx in range(len(nodes.keys())):\n",
    "    ax.annotate(idx+1, (x1[idx]-0.0025,  y1[idx]-0.002))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show TF-IDF normalized vocab of a specific entry (25 most frequent words)\n",
    "tmp_array = -wc_array2[8,:]\n",
    "top_ranked = np.argsort(-tmp_array)\n",
    "limiter = 0\n",
    "for i in top_ranked:\n",
    "    print(tmp_array[i],\":\",vocab[i])\n",
    "    limiter=limiter+1\n",
    "    if limiter>25:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_reviews['text'].values[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF exploration and display\n",
    "# - use same preprocessing, but get raw counts so we can show effects of normalization\n",
    "\n",
    "# Vectorize term counts, for exploration/display\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "\n",
    "stopwords = routedtools.get_stop_words(\"./resources/stopwords.txt\")\n",
    "nearby_stores,nearby_reviews = routedtools.get_nearby(start_lat, start_long, radius, \n",
    "                                       't_phoenix', 't_review_phx', conn, min_length)\n",
    "\n",
    "docs=nearby_reviews['text'].tolist()\n",
    "\n",
    "cv=CountVectorizer(max_df=0.85,stop_words=stopwords)\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "wc_array = word_count_vector.toarray()\n",
    "vocab = cv.get_feature_names()\n",
    "\n",
    "\n",
    "\n",
    "# Show vocab of a specific entry (25 most frequent words)\n",
    "vocab = cv.get_feature_names()\n",
    "tmp_array = wc_array[8,:]\n",
    "top_ranked = np.argsort(-tmp_array)\n",
    "limiter = 0\n",
    "for i in top_ranked:\n",
    "    print(tmp_array[i],\":\",vocab[i])\n",
    "    limiter=limiter+1\n",
    "    if limiter>25:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make TF-IDF scatterplot examples\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "vals = np.random.randint(1693, size=4)\n",
    "for i in vals:\n",
    "    idf = np.log((1/(np.mean(wc_array>0,axis=0))))\n",
    "    tf1 = (wc_array[i,:])/np.sum(wc_array[i,:])\n",
    "    # Draw Plot for Each Categor6\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    #plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    ax.scatter((idf),(tf1),80,alpha=0.2)\n",
    "    for j, txt in enumerate(vocab):\n",
    "        if (tf1[j]>0.005)and(wc_array[i,j]>1):\n",
    "            ax.annotate(txt, (idf[j], tf1[j]),size=16)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
